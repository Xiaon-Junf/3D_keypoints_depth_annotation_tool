# ------------------------------------------------------------------------------
# ç›¸æœºåæ ‡è½¬æ¢å·¥å…·å‡½æ•°
# å‚è€ƒMM_GCNå®ç°ï¼Œå¤ç”¨simple_depth_readerçš„ç›¸æœºå‚æ•°
# ------------------------------------------------------------------------------

import numpy as np
from typing import Dict, Tuple
from utils.simple_depth_reader import SimpleDepthReader

# PyTorchä¸ºå¯é€‰ä¾èµ–ï¼Œä»…åœ¨éœ€è¦æ—¶å¯¼å…¥
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None


def pixel2cam(pixel_coord: np.ndarray, f: np.ndarray, c: np.ndarray) -> np.ndarray:
    """
    å°†åƒç´ åæ ‡+æ·±åº¦è½¬æ¢ä¸ºç›¸æœºåæ ‡ç³»3Dåæ ‡ï¼ˆå‚è€ƒMM_GCNå®ç°ï¼‰
    
    Args:
        pixel_coord (np.ndarray): åƒç´ åæ ‡+æ·±åº¦ [N, 3] æ ¼å¼ä¸º (u, v, z_depth)
        f (np.ndarray): ç„¦è·å‚æ•° [fx, fy]
        c (np.ndarray): ä¸»ç‚¹å‚æ•° [cx, cy]
        
    Returns:
        np.ndarray: ç›¸æœºåæ ‡ç³»3Dåæ ‡ [N, 3] æ ¼å¼ä¸º (X_cam, Y_cam, Z_cam)
    """
    # MM_GCNçš„å®ç°
    x = (pixel_coord[:, 0] - c[0]) / f[0] * pixel_coord[:, 2]
    y = (pixel_coord[:, 1] - c[1]) / f[1] * pixel_coord[:, 2]
    z = pixel_coord[:, 2]
    cam_coord = np.concatenate((x[:, None], y[:, None], z[:, None]), 1)
    return cam_coord


def cam2pixel(cam_coord: np.ndarray, f: np.ndarray, c: np.ndarray) -> np.ndarray:
    """
    å°†ç›¸æœºåæ ‡ç³»3Dåæ ‡è½¬æ¢ä¸ºåƒç´ åæ ‡+æ·±åº¦ï¼ˆå‚è€ƒMM_GCNå®ç°ï¼‰
    
    Args:
        cam_coord (np.ndarray): ç›¸æœºåæ ‡ç³»3Dåæ ‡ [N, 3] æ ¼å¼ä¸º (X_cam, Y_cam, Z_cam)
        f (np.ndarray): ç„¦è·å‚æ•° [fx, fy]  
        c (np.ndarray): ä¸»ç‚¹å‚æ•° [cx, cy]
        
    Returns:
        np.ndarray: åƒç´ åæ ‡+æ·±åº¦ [N, 3] æ ¼å¼ä¸º (u, v, z_depth)
    """
    # MM_GCNçš„å®ç°
    x = cam_coord[:, 0] / (cam_coord[:, 2] + 1e-8) * f[0] + c[0]
    y = cam_coord[:, 1] / (cam_coord[:, 2] + 1e-8) * f[1] + c[1]
    z = cam_coord[:, 2]
    img_coord = np.concatenate((x[:, None], y[:, None], z[:, None]), 1)
    return img_coord


def convert_joints_to_camera_coords(joints_pixel_depth: np.ndarray, 
                                   depth_reader: SimpleDepthReader) -> np.ndarray:
    """
    ä½¿ç”¨ç°æœ‰SimpleDepthReaderçš„ç›¸æœºå‚æ•°è½¬æ¢å…³é”®ç‚¹åæ ‡
    
    Args:
        joints_pixel_depth (np.ndarray): å…³é”®ç‚¹åƒç´ åæ ‡+æ·±åº¦ [num_joints, 3]
        depth_reader (SimpleDepthReader): ç°æœ‰çš„æ·±åº¦è¯»å–å™¨å®ä¾‹
        
    Returns:
        np.ndarray: ç›¸æœºåæ ‡ç³»å…³é”®ç‚¹ [num_joints, 3]
    """
    # ç›´æ¥ä½¿ç”¨ç°æœ‰çš„ç›¸æœºå‚æ•°
    fx = depth_reader.camera_params['fx']
    fy = depth_reader.camera_params['fy']
    cx = depth_reader.camera_params['cx1']  # æ³¨æ„ï¼šç°æœ‰çš„æ˜¯cx1
    cy = depth_reader.camera_params['cy']
    
    f = np.array([fx, fy], dtype=np.float32)
    c = np.array([cx, cy], dtype=np.float32)
    
    return pixel2cam(joints_pixel_depth, f, c)


def convert_joints_from_camera_coords(joints_camera: np.ndarray, 
                                     depth_reader: SimpleDepthReader) -> np.ndarray:
    """
    å°†ç›¸æœºåæ ‡ç³»å…³é”®ç‚¹è½¬æ¢å›åƒç´ åæ ‡+æ·±åº¦ï¼ˆé€†å˜æ¢ï¼‰
    
    Args:
        joints_camera (np.ndarray): ç›¸æœºåæ ‡ç³»å…³é”®ç‚¹ [num_joints, 3] (X_cam, Y_cam, Z_cam)
        depth_reader (SimpleDepthReader): ç°æœ‰çš„æ·±åº¦è¯»å–å™¨å®ä¾‹
        
    Returns:
        np.ndarray: åƒç´ åæ ‡+æ·±åº¦ [num_joints, 3] (u, v, z)
        
    Note:
        - ç”¨äºè®­ç»ƒæ—¶å°†ç›¸æœºåæ ‡GTè½¬æ¢å›åƒç´ åæ ‡ï¼Œç¡®ä¿ä¸HRNeté¢„æµ‹çš„åæ ‡ç©ºé—´ä¸€è‡´
        - é®æŒ¡ç‚¹(0,0,0)ç›¸æœºåæ ‡ä¼šè½¬æ¢ä¸º(cx, cy, 0)åƒç´ åæ ‡ï¼Œä¿æŒåˆç†æ€§
    """
    # å¤ç”¨ç°æœ‰çš„ç›¸æœºå‚æ•°
    fx = depth_reader.camera_params['fx']
    fy = depth_reader.camera_params['fy']
    cx = depth_reader.camera_params['cx1']  # æ³¨æ„ï¼šç°æœ‰çš„æ˜¯cx1
    cy = depth_reader.camera_params['cy']
    
    f = np.array([fx, fy], dtype=np.float32)
    c = np.array([cx, cy], dtype=np.float32)
    
    return cam2pixel(joints_camera, f, c)


def torch_cam2pixel(cam_coord, f, c):
    """
    PyTorchç‰ˆæœ¬çš„ç›¸æœºåæ ‡åˆ°åƒç´ åæ ‡è½¬æ¢ï¼ˆç”¨äºè®­ç»ƒä¸­çš„é€†å˜æ¢ï¼‰
    
    Args:
        cam_coord (torch.Tensor): ç›¸æœºåæ ‡ [B, N, 3] æ ¼å¼ä¸º (X_cam, Y_cam, Z_cam)
        f (torch.Tensor): ç„¦è·å‚æ•° [B, 2] æˆ– [2]
        c (torch.Tensor): ä¸»ç‚¹å‚æ•° [B, 2] æˆ– [2]
        
    Returns:
        torch.Tensor: åƒç´ åæ ‡+æ·±åº¦ [B, N, 3] æ ¼å¼ä¸º (u, v, z_depth)
    """
    if not TORCH_AVAILABLE:
        raise ImportError("PyTorchä¸å¯ç”¨")
    
    # ç¡®ä¿få’Œcçš„ç»´åº¦æ­£ç¡®
    if len(f.shape) == 1:
        f = f.unsqueeze(0)  # [2] -> [1, 2]
    if len(c.shape) == 1:
        c = c.unsqueeze(0)  # [2] -> [1, 2]
    
    # å¤„ç†æ‰¹æ¬¡ç»´åº¦
    if f.shape[0] == 1 and cam_coord.shape[0] > 1:
        f = f.repeat(cam_coord.shape[0], 1)  # [1, 2] -> [B, 2]
    if c.shape[0] == 1 and cam_coord.shape[0] > 1:
        c = c.repeat(cam_coord.shape[0], 1)  # [1, 2] -> [B, 2]
    
    # ç›¸æœºåæ ‡åˆ°åƒç´ åæ ‡çš„æŠ•å½±å˜æ¢
    # æ·»åŠ å°å€¼é¿å…é™¤é›¶
    z_safe = cam_coord[:, :, 2:3] + 1e-8
    x = cam_coord[:, :, 0:1] / z_safe * f[:, 0:1].unsqueeze(1) + c[:, 0:1].unsqueeze(1)
    y = cam_coord[:, :, 1:2] / z_safe * f[:, 1:2].unsqueeze(1) + c[:, 1:2].unsqueeze(1)
    z = cam_coord[:, :, 2:3]
    
    return torch.cat([x, y, z], dim=-1)


def torch_pixel2cam(pixel_coord, f, c):
    """
    PyTorchç‰ˆæœ¬çš„åƒç´ åæ ‡åˆ°ç›¸æœºåæ ‡è½¬æ¢ï¼ˆå‚è€ƒMM_GCNï¼‰
    """
    if not TORCH_AVAILABLE:
        raise ImportError("PyTorchä¸å¯ç”¨")
    
    x = (pixel_coord[:, :, 0] - c[:, 0:1]) / f[:, 0:1] * pixel_coord[:, :, 2]
    y = (pixel_coord[:, :, 1] - c[:, 1:2]) / f[:, 1:2] * pixel_coord[:, :, 2]
    z = pixel_coord[:, :, 2]
    
    return torch.stack([x, y, z], dim=-1)


def sample_z_init_from_hrnet_and_depth(hrnet_coords: 'torch.Tensor', 
                                       depth_input: 'torch.Tensor',
                                       depth_reader: SimpleDepthReader,
                                       depth_max_value: float = 10000.0,
                                       interpolation_mode: str = 'nearest') -> 'torch.Tensor':
    """
    åŸºäºHRNetè¾“å‡ºåæ ‡å’Œæ·±åº¦å›¾è®¡ç®—z_initï¼ˆå½’ä¸€åŒ–çš„ç›¸æœºåæ ‡ç³»æ·±åº¦å€¼ï¼‰
    
    å®ç°å®Œæ•´æµç¨‹ï¼š
    1. HRNetè¾“å‡ºxy â†’ åƒç´ åæ ‡ï¼ˆé€†å½’ä¸€åŒ–ï¼‰
    2. åœ¨æ·±åº¦å›¾ä¸Šé‡‡æ ·æ·±åº¦å€¼
    3. è½¬æ¢ä¸ºç›¸æœºåæ ‡ç³»
    4. ä½¿ç”¨simple_depth_readerçš„å½’ä¸€åŒ–ç­–ç•¥
    
    Args:
        hrnet_coords (torch.Tensor): HRNetè¾“å‡ºçš„å½’ä¸€åŒ–åæ ‡ [B, 17, 2] èŒƒå›´çº¦[-1,1]
        depth_input (torch.Tensor): æ·±åº¦å›¾ [B, 1, H, W] å·²å½’ä¸€åŒ–åˆ°[0,1]
        depth_reader (SimpleDepthReader): ç›¸æœºå‚æ•°å’Œå½’ä¸€åŒ–å·¥å…·
        depth_max_value (float): æœ€å¤§æ·±åº¦å€¼ï¼Œç”¨äºé€†å½’ä¸€åŒ–
        interpolation_mode (str): æ’å€¼æ¨¡å¼ï¼Œ'nearest'æˆ–'bilinear'
        
    Returns:
        torch.Tensor: å½’ä¸€åŒ–çš„zåæ ‡ [B, 17, 1] èŒƒå›´[0,1]
    """
    if not TORCH_AVAILABLE:
        raise ImportError("PyTorchä¸å¯ç”¨")
    
    B, num_joints, _ = hrnet_coords.shape
    _, _, H, W = depth_input.shape
    device = hrnet_coords.device
    
    # ğŸ”„ æ­¥éª¤1: å°†HRNetå½’ä¸€åŒ–åæ ‡è½¬æ¢ä¸ºæ·±åº¦å›¾åƒç´ åæ ‡
    # HRNetè¾“å‡ºå¤§çº¦åœ¨[-1, 1]èŒƒå›´ï¼Œéœ€è¦è½¬æ¢åˆ°[0, H-1]å’Œ[0, W-1]
    pixel_coords = hrnet_coords.clone()
    pixel_coords[:, :, 0] = (pixel_coords[:, :, 0] + 1) * (W - 1) / 2  # x: [-1,1] -> [0, W-1]
    pixel_coords[:, :, 1] = (pixel_coords[:, :, 1] + 1) * (H - 1) / 2  # y: [-1,1] -> [0, H-1]
    
    # ğŸ¯ æ­¥éª¤2: åœ¨æ·±åº¦å›¾ä¸Šé‡‡æ ·æ·±åº¦å€¼ï¼ˆä½¿ç”¨grid_sampleåŒçº¿æ€§æ’å€¼ï¼‰
    # è½¬æ¢ä¸ºgrid_sampleæœŸæœ›çš„æ ¼å¼[-1, 1]
    grid = pixel_coords.clone()
    grid[:, :, 0] = 2 * grid[:, :, 0] / (W - 1) - 1  # x: [0, W-1] -> [-1, 1]
    grid[:, :, 1] = 2 * grid[:, :, 1] / (H - 1) - 1  # y: [0, H-1] -> [-1, 1]
    
    # ğŸ¯ æ‰§è¡Œæ·±åº¦é‡‡æ ·ï¼ˆæ”¯æŒé…ç½®çš„æ’å€¼æ¨¡å¼ï¼‰
    # nearest: ä¿æŒçœŸå®æ·±åº¦å€¼ï¼Œé¿å…è·¨ç‰©ä½“æ·±åº¦æ··åˆ
    # bilinear: å¹³æ»‘æ’å€¼ï¼Œå¯èƒ½åœ¨æŸäº›åœºæ™¯ä¸‹æä¾›æ›´å¥½çš„è¿ç»­æ€§
    grid_sample_format = grid.unsqueeze(2)  # [B, 17, 1, 2]
    sampled_depth_normalized = torch.nn.functional.grid_sample(
        depth_input, grid_sample_format, 
        align_corners=True, mode=interpolation_mode, padding_mode='border'
    )  # [B, 1, 17, 1]
    
    sampled_depth_normalized = sampled_depth_normalized.squeeze(1).transpose(1, 2)  # [B, 17, 1]
    
    # ğŸ”„ æ­¥éª¤3: é€†å½’ä¸€åŒ–æ·±åº¦å€¼åˆ°æ¯«ç±³å•ä½ï¼ˆä¸simple_depth_readerä¸€è‡´ï¼‰
    sampled_depth_mm = sampled_depth_normalized * depth_max_value  # [B, 17, 1]
    
    # ğŸ—ï¸ æ­¥éª¤4: æ„å»ºå®Œæ•´çš„åƒç´ +æ·±åº¦åæ ‡ç”¨äºè½¬æ¢
    # éœ€è¦å°†åƒç´ åæ ‡è½¬æ¢å›åŸå§‹å›¾åƒåæ ‡ç³»ï¼ˆéå½’ä¸€åŒ–ï¼‰ç”¨äºç›¸æœºåæ ‡è½¬æ¢
    fx = depth_reader.camera_params['fx']
    fy = depth_reader.camera_params['fy']
    cx = depth_reader.camera_params['cx1']
    cy = depth_reader.camera_params['cy']
    
    # åˆ›å»ºç›¸æœºå‚æ•°å¼ é‡
    f = torch.tensor([fx, fy], dtype=torch.float32, device=device)
    c = torch.tensor([cx, cy], dtype=torch.float32, device=device)
    
    # ğŸ”„ æ­¥éª¤5: å°†åƒç´ åæ ‡è½¬æ¢ä¸ºç›¸æœºåæ ‡ï¼ˆä»…ä¸ºäº†è·å¾—ç‰©ç†æ„ä¹‰çš„zå€¼ï¼‰
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¸»è¦å…³å¿ƒzåæ ‡çš„ç‰©ç†æ„ä¹‰ï¼Œxyåæ ‡ä¼šåœ¨åç»­normalizeä¸­å¤„ç†
    pixel_coords_with_depth = torch.cat([pixel_coords, sampled_depth_mm], dim=-1)  # [B, 17, 3]
    
    # è½¬æ¢ä¸ºç›¸æœºåæ ‡ç³»ï¼ˆè·å¾—ç‰©ç†æ„ä¹‰çš„æ·±åº¦ï¼‰
    camera_coords = torch.zeros_like(pixel_coords_with_depth)
    for b in range(B):
        # åªå¤„ç†æœ‰æœ‰æ•ˆæ·±åº¦çš„ç‚¹
        valid_mask = sampled_depth_mm[b, :, 0] > 0
        if valid_mask.any():
            valid_pixels = pixel_coords_with_depth[b][valid_mask]  # [N_valid, 3]
            # ä½¿ç”¨torchç‰ˆæœ¬çš„è½¬æ¢å‡½æ•°
            f_batch = f.unsqueeze(0)  # [1, 2]
            c_batch = c.unsqueeze(0)  # [1, 2]
            valid_pixels_batch = valid_pixels.unsqueeze(0)  # [1, N_valid, 3]
            valid_camera = torch_pixel2cam(valid_pixels_batch, f_batch, c_batch)[0]  # [N_valid, 3]
            camera_coords[b][valid_mask] = valid_camera
    
    # ğŸ¯ æ­¥éª¤6: ä½¿ç”¨simple_depth_readerçš„å½’ä¸€åŒ–ç­–ç•¥å¤„ç†zåæ ‡
    # æå–zåæ ‡ï¼ˆç›¸æœºåæ ‡ç³»çš„å‰å‘æ·±åº¦ï¼‰
    z_camera_mm = camera_coords[:, :, 2:3]  # [B, 17, 1]
    
    # ä½¿ç”¨ä¸simple_depth_readerç›¸åŒçš„å½’ä¸€åŒ–ç­–ç•¥
    z_normalized = torch.clamp(z_camera_mm, 0, depth_max_value) / depth_max_value  # [B, 17, 1]

    return z_normalized


def project_left_to_right(x_left: float, y_left: float, depth: float,
                          fx: float, baseline: float) -> Tuple[float, float]:
    """
    å°†å·¦å›¾åƒç´ åæ ‡æŠ•å½±åˆ°å³å›¾ï¼ˆæçº¿æ ¡å‡†åçš„ç®€åŒ–ç‰ˆæœ¬ï¼‰

    ç”±äºå›¾åƒå·²ç»è¿‡æçº¿æ ¡å‡†ï¼ˆrectifiedï¼‰ï¼Œæçº¿å˜ä¸ºæ°´å¹³çº¿ï¼Œ
    åŒä¸€3Dç‚¹åœ¨å·¦å³å›¾çš„yåæ ‡ç›¸åŒï¼Œåªæœ‰xåæ ‡å› è§†å·®è€Œä¸åŒã€‚

    Args:
        x_left: å·¦å›¾xåæ ‡ï¼ˆåƒç´ ï¼‰
        y_left: å·¦å›¾yåæ ‡ï¼ˆåƒç´ ï¼‰
        depth: æ·±åº¦å€¼ï¼ˆmmï¼‰
        fx: å·¦ç›¸æœºç„¦è·ï¼ˆåƒç´ ï¼‰
        baseline: åŸºçº¿è·ç¦»ï¼ˆmmï¼‰ï¼Œå³å·¦å³ç›¸æœºå…‰å¿ƒä¹‹é—´çš„è·ç¦»

    Returns:
        (x_right, y_right): å³å›¾åæ ‡ï¼ˆåƒç´ ï¼‰

    Note:
        è§†å·®å…¬å¼: disparity = (fx * baseline) / depth
        å³å›¾xåæ ‡: x_right = x_left - disparity
        å³å›¾yåæ ‡: y_right = y_leftï¼ˆæçº¿æ ¡å‡†åç›¸åŒï¼‰
    """
    if depth <= 0:
        # æ·±åº¦æ— æ•ˆæ—¶è¿”å›åŸåæ ‡
        return x_left, y_left

    # è®¡ç®—è§†å·®
    disparity = (fx * baseline) / depth

    # è®¡ç®—å³å›¾åæ ‡
    x_right = x_left - disparity
    y_right = y_left  # æçº¿æ ¡å‡†åyåæ ‡ç›¸åŒ

    return x_right, y_right


def project_keypoints_left_to_right(keypoints: Dict[str, list],
                                    fx: float,
                                    baseline: float) -> Dict[str, Tuple[float, float]]:
    """
    æ‰¹é‡å°†å·¦å›¾å…³é”®ç‚¹æŠ•å½±åˆ°å³å›¾

    Args:
        keypoints: å…³é”®ç‚¹å­—å…¸ï¼Œæ ¼å¼ä¸º {name: [x, y, depth]}
        fx: å·¦ç›¸æœºç„¦è·ï¼ˆåƒç´ ï¼‰
        baseline: åŸºçº¿è·ç¦»ï¼ˆmmï¼‰

    Returns:
        å³å›¾å…³é”®ç‚¹å­—å…¸ï¼Œæ ¼å¼ä¸º {name: (x_right, y_right)}
    """
    right_keypoints = {}
    for name, kp in keypoints.items():
        x_left, y_left, depth = kp[0], kp[1], kp[2]
        x_right, y_right = project_left_to_right(x_left, y_left, depth, fx, baseline)
        right_keypoints[name] = (x_right, y_right)
    return right_keypoints
